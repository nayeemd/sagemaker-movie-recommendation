{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Taxi Fare Prediction using SageMaker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This demo notebook guides through the steps in creating an AWS Sagemaker predictions service</p>\n",
    "<p>To start using Sagemaker go here: <a href=\"https://aws.amazon.com/sagemaker/\">Sagemaker</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the whole data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'data-ml-training'\n",
    "path = 'data/train/'\n",
    "filename = 'Training_data_cleaned.csv'\n",
    "s3_path = os.path.join('s3://', bucket, path, filename)\n",
    "\n",
    "df_all = pd.read_csv(s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Exploring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data - because it only take a 100K records to feel the data\n",
    "df_all = df_all.sample(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>449</th>\n",
       "      <th>146</th>\n",
       "      <th>355</th>\n",
       "      <th>1988</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>98512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.981722e+02</td>\n",
       "      <td>669.980150</td>\n",
       "      <td>991.671910</td>\n",
       "      <td>1977.618361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.633855e+04</td>\n",
       "      <td>835.151032</td>\n",
       "      <td>1176.720637</td>\n",
       "      <td>12.734852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.100000e+01</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.170000e+02</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>1969.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.320000e+02</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>458.000000</td>\n",
       "      <td>1979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.700000e+02</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.845321e+06</td>\n",
       "      <td>3158.000000</td>\n",
       "      <td>3684.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                449            146            355          1988\n",
       "count  1.000000e+05  100000.000000  100000.000000  98512.000000\n",
       "mean   7.981722e+02     669.980150     991.671910   1977.618361\n",
       "std    1.633855e+04     835.151032    1176.720637     12.734852\n",
       "min    6.100000e+01     146.000000      72.000000   1900.000000\n",
       "25%    3.170000e+02     291.000000     334.000000   1969.000000\n",
       "50%    5.320000e+02     470.000000     458.000000   1979.000000\n",
       "75%    8.700000e+02     519.000000     529.000000   1988.000000\n",
       "max    4.845321e+06    3158.000000    3684.000000   2002.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-Time Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['ts'] = df_all['pickup_datetime'].apply(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['weekday'] = df_all['ts'].dt.weekday\n",
    "df_all['day'] = df_all['ts'].dt.day\n",
    "df_all['month'] = df_all['ts'].dt.month\n",
    "df_all['year'] = df_all['ts'].dt.year\n",
    "df_all['hour'] = df_all['ts'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['day', 'month', 'weekday', 'year', 'hour']\n",
    "\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(len(time_features)):\n",
    "    ax = fig.add_subplot(2, 3, i+1)\n",
    "    sns.boxplot(x=time_features[i], y=\"fare_amount\", data=df_all[df_all['year'] >= 2013], showfliers=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_box = (-74.1, -73.7, 40.6, 40.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_coordinates(df, box):\n",
    "    return (df.pickup_longitude >= box[0]) & (df.pickup_longitude <= box[1]) & \\\n",
    "           (df.pickup_latitude >= box[2]) & (df.pickup_latitude <=box[3]) & \\\n",
    "           (df.dropoff_longitude >= box[0]) & (df.dropoff_longitude <= box[1]) & \\\n",
    "           (df.dropoff_latitude >= box[2]) & (df.dropoff_latitude <= box[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hires(df, box, figsize=(12, 12), ax=None, c=sns.color_palette(\"husl\", 2)):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    idx = filter_coordinates(df, box)\n",
    "    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=c[0], s=0.1, lw=0.1, alpha=0.5)\n",
    "    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=c[1], s=0.1, lw=0.1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df_all, nyc_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the scatter plot above, the two major NYC airports are easily observed, indicating that it may be informative to \"tell\" our algorithm whether the ride starts or ends in an airport.</p>\n",
    "<p>To do so I use the airports.csv, available <a href=\"http://ourairports.com/data/\">here</a> in the following way:</p>\n",
    "<ul>\n",
    "<li>from this file I extract the location of 3 of the listed NYC airports</li>\n",
    "<li>I than calculate the distance of dropoff and pickup points from every record to each airport, Assuming it will provide additional information to our model.</li>\n",
    "</ul>\n",
    "<p>Further details are in the feature extraction below</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ride distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['air_distance'] = (df_all.pickup_longitude - df_all.dropoff_longitude)**2 +\\\n",
    "                     (df_all.pickup_latitude - df_all.dropoff_latitude)**2\n",
    "df_all['air_distance'] = np.sqrt(df_all['air_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['price_per_distance'] = df_all['fare_amount'] / df_all['air_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['weekday', 'year', 'hour']\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(len(time_features)):\n",
    "    ax = fig.add_subplot(1, 3, i+1)\n",
    "    sns.boxplot(x=time_features[i], y=\"price_per_distance\", data=df_all[df_all['year'] >= 2013], showfliers=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['weekday', 'year', 'hour']\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(len(time_features)):\n",
    "    ax = fig.add_subplot(1, 3, i+1)\n",
    "    sns.boxplot(x=time_features[i], y=\"air_distance\", data=df_all[df_all['year'] >= 2013], showfliers=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Athena to extract features on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As mentioned, the dataset we are working with contains 55M records, making its handling too heavy for a single machine.</p>\n",
    "<p>Using a distributed computing engine like&nbsp;<a href=\"https://aws.amazon.com/athena/\">AWS Athena</a>&nbsp;will enable you to extract features and save data efficiently.&nbsp;</p>\n",
    "<p>In order to work on the data, we upload it to S3, and than partition it using AWS Glue. Partitioning is critical to make Athena run efficiently. For examples on how to use Glue, go&nbsp;<a href=\"https://github.com/doitintl/aws-glue-workshop\">HERE</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the data partitioned (say, by year and month), run the following Athena query to extract the following features&nbsp;</p>\n",
    "<p>After extracting features, partition the query results using Glue (again)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = '''\n",
    "WITH \n",
    "    dataset AS \n",
    "    (SELECT CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York' AS est, \n",
    "                  ST_POINT(pickup_longitude,pickup_latitude) pickup_point,\n",
    "                  ST_POINT(dropoff_longitude,dropoff_latitude) dropoff_point,\n",
    "                  to_unixtime( CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York') AS                     epoch,\n",
    "                  24*60*60 as seconds_in_day,\n",
    "                  *\n",
    "      FROM train_v3),\n",
    "    \n",
    "    airports AS (SELECT \n",
    "                  kv['LaGuardia'] AS LaGuardia,\n",
    "                  kv['Downtown Manhattan/Wall St. Heliport'] AS Manhattan,\n",
    "                  kv['John F Kennedy Intl'] AS JFK\n",
    "    FROM (SELECT map_agg(name, point_location) kv\n",
    "        FROM \n",
    "            (SELECT name,\n",
    "         ST_POINT(longitude,\n",
    "         latitude) point_location\n",
    "            FROM usa_airports\n",
    "            WHERE city = 'New York' )\n",
    "            ))\n",
    "        SELECT \n",
    "        \n",
    "        -- Target\n",
    "         fare_amount,\n",
    "         \n",
    "         -- time features\n",
    "         day(est) day,\n",
    "         day_of_week(est) dayofweek ,\n",
    "         year(est) year ,\n",
    "         month(est) month ,\n",
    "         day_of_month(est) dayofmonth ,\n",
    "         hour(est) hour ,\n",
    "         minute(est) minute ,\n",
    "         \n",
    "         -- cyclclical variables\n",
    "         sin(2*pi()*epoch/seconds_in_day) sin_day,\n",
    "         cos(2*pi()*epoch/seconds_in_day) cos_day,\n",
    "         sin(2*pi()*epoch/(seconds_in_day*7)) sin_week,\n",
    "         cos(2*pi()*epoch/(seconds_in_day*7)) cos_week,\n",
    "         \n",
    "         \n",
    "         -- Distance features\n",
    "         pickup_longitude - dropoff_longitude diff_longitude,\n",
    "         pickup_latitude - dropoff_latitude diff_latitude,\n",
    "         ST_Distance(pickup_point, dropoff_point) dist,\n",
    "         \n",
    "         -- Airports features\n",
    "         ST_DISTANCE(airports.LaGuardia, dropoff_point) dropoff_laguardia,\n",
    "         ST_DISTANCE(airports.LaGuardia, pickup_point ) pickup_laguardia,\n",
    "         ST_DISTANCE(airports.JFK, dropoff_point) dropoff_JFK,\n",
    "         ST_DISTANCE(airports.JFK, pickup_point) pickup_JFK,\n",
    "         ST_DISTANCE(airports.Manhattan, dropoff_point) dropoff_manhattan,\n",
    "         ST_DISTANCE(airports.Manhattan, pickup_point) pickup_manhattan,\n",
    "         \n",
    "         -- Raw features\n",
    "         pickup_longitude,\n",
    "         pickup_latitude,\n",
    "         dropoff_longitude,\n",
    "         dropoff_latitude,\n",
    "         passenger_count\n",
    "         \n",
    "    FROM dataset, airports\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you managed to create the dataset using Athena, you have two options on how to create train and validation sets files.&nbsp;</p>\n",
    "<ol>\n",
    "<li>Create a seperate query, using a WHERE on the year clause to split the train and validation by time (e.g. year, month)</li>\n",
    "<li>Create a single query, partition the results by time, and use aws-cli to mv the directories to different train/validation directotires</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the second option\n",
    "!aws s3 mv s3://athena-tmp-results/..../dataset/year=2015 s3://<my_bucket>/.../dataset/train/year=2015/\n",
    "!aws s3 mv s3://athena-tmp-results/..../dataset/year=2014 s3://<my_bucket>/.../dataset/val/year=2014/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join('sagemaker_demo','data_2','train')\n",
    "path_val = os.path.join('sagemaker_demo','data_2','val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_trains3_inpu  = sagemaker.s3_input(s3_data='s3://{}/{}'.format(bucket, path_train),\n",
    "                                            content_type='csv',\n",
    "                                            distribution='ShardedByS3Key')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}'.format(bucket, path_val),\n",
    "                                         content_type='csv', \n",
    "                                         distribution='ShardedByS3Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_path = os.path.join('training_jobs', datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=4, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket,training_job_path),\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'s3://{}/{}/output'.format(bucket,training_job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=9,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=300,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_trains3_inpu, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model we move to the Amazon Sagemaker console and perform the following steps: <br>\n",
    "1) Create model <br>\n",
    "2) Create an endpoint configuration <br>\n",
    "3) create and endpoint <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts taken from here:<br>\n",
    "https://aws.amazon.com/blogs/machine-learning/simplify-machine-learning-with-xgboost-and-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./df_val.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(df_test.as_matrix()[:, 1:])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame()\n",
    "df_val['prediction'] = predictions\n",
    "df_val['target'] = df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.regplot(x=df_val[\"target\"], y=df_val[\"prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['err'] = (df_val[\"target\"] - df_val[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['err'].plot(kind='hist', bins=100, range=(-6,10), alpha=0.8, figsize=(15,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make online predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join('s3://', bucket, path_val, 'year=2015', 'month=4','run-1534713776930-part-r-00000')\n",
    "df = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "n = 20\n",
    "features = df.iloc[0:n,1:].values\n",
    "values  = df.iloc[0:n,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'xgboost-2018-08-20-19-04-07-244'\n",
    "content_type = 'text/csv'\n",
    "data = \"\\n\".join([\"\\n\".join([\",\".join(str(x) for x in sample)]) for sample in features])\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\\\n",
    "                                   ContentType='text/csv',\\\n",
    "                                   Body=data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(ast.literal_eval(response['Body'].read().decode()))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
